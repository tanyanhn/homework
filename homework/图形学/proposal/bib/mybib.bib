@article{Werbos1990,
abstract = {Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. {\textcopyright} 1990, IEEE},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {10},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
@book{S2009Neural,
author = {Haykin, S S and Gwynn, R},
publisher = {China Machine Press,},
title = {{Neural Networks and Learning Machines}},
year = {2009}
}
@inproceedings{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors. Copyright 2010 by the author(s)/owner(s).},
author = {Nair, Vinod and Hinton, Geoffrey E.},
booktitle = {ICML 2010 - Proceedings, 27th International Conference on Machine Learning},
isbn = {9781605589077},
pages = {807--814},
title = {{Rectified linear units improve Restricted Boltzmann machines}},
year = {2010}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{Hinton2006,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G. E. and Salakhutdinov, R. R.},
doi = {10.1126/science.1127647},
issn = {00368075},
journal = {Science},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the dimensionality of data with neural networks}},
volume = {313},
year = {2006}
}
@article{Werbos1974,
abstract = {Early, extensive studies of feed-forward connectionist networks of analog units with sigmoid activation function AL 3/1/2004.},
author = {Werbos, P.J.},
doi = {10.1.1.41.8085},
journal = {PhD Thesis, Harvard U.},
number = {January 1974},
title = {{Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences}},
volume = {PhD thesis},
year = {1974}
}
@article{Williams1989,
abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1162/neco.1989.1.2.270},
issn = {0899-7667},
journal = {Neural Computation},
month = {jun},
number = {2},
pages = {270--280},
publisher = {MIT Press - Journals},
title = {{A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}},
volume = {1},
year = {1989}
}
@article{LeCun1989,
abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the US Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
doi = {10.1162/neco.1989.1.4.541},
issn = {0899-7667},
journal = {Neural Computation},
month = {dec},
number = {4},
pages = {541--551},
publisher = {MIT Press - Journals},
title = {{Backpropagation Applied to Handwritten Zip Code Recognition}},
volume = {1},
year = {1989}
}
@article{ROSENBLATT1958,
author = {ROSENBLATT, F},
doi = {10.1037/h0042519},
issn = {0033-295X (Print)},
journal = {Psychological review},
keywords = {Brain,Humans,Information Storage and Retrieval,Models, Statistical,Neural Networks, Computer,Perception},
language = {eng},
month = {nov},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: a probabilistic model for information storage and organization in  the brain.}},
volume = {65},
year = {1958}
}
@book{McClelland1986,
abstract = {The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.},
author = {McClelland, James L. and Rumelhart, David E. and McClelland, James L.},
booktitle = {Parallel Distributed Processing Explorations in the Microstructure of Cognition Volume 1 Foundations},
isbn = {0262132184},
pages = {567},
pmid = {108},
publisher = {MIT Press},
title = {{Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 2: Psychological and Biological Models}},
url = {http://www.osti.gov/energycitations/product.biblio.jsp?osti{\_}id=5838709},
volume = {1},
year = {1986}
}
